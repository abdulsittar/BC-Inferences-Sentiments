{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aba8b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abdul\n",
      "[nltk_data]     Sittar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abdul\n",
      "[nltk_data]     Sittar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "import sys, os\n",
    "from transformers import BertTokenizer\n",
    "from textwrap3 import wrap\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d9b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"\"\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b302398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading BERT Tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "data = pd.read_csv('C:/Users/Abdul Sittar/Conda/Title-Data/Home-Economic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5616694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model3 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b08700e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home-Economic\n",
      "im here\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "im here\n",
      "168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABDULS~1\\AppData\\Local\\Temp/ipykernel_9692/4267138561.py:17: FutureWarning: In a future version of pandas all arguments of Series.dropna will be keyword-only.\n",
      "  Features['title'] = Features['title'].dropna(\"\").reset_index(drop=True)\n",
      "C:\\Users\\ABDULS~1\\AppData\\Local\\Temp/ipykernel_9692/4267138561.py:19: FutureWarning: In a future version of pandas all arguments of Series.dropna will be keyword-only.\n",
      "  Features['senti-str-headline'] = Features['senti-str-headline'].dropna(\"\").reset_index(drop=True)\n",
      "C:\\Users\\ABDULS~1\\AppData\\Local\\Temp/ipykernel_9692/4267138561.py:21: FutureWarning: In a future version of pandas all arguments of Series.dropna will be keyword-only.\n",
      "  Features['allinferences'] = Features['allinferences'].dropna(\"\").reset_index(drop=True)\n",
      "C:\\Users\\ABDULS~1\\AppData\\Local\\Temp/ipykernel_9692/4267138561.py:23: FutureWarning: In a future version of pandas all arguments of Series.dropna will be keyword-only.\n",
      "  Features['class'] = Features['class'].dropna(\"\").reset_index(drop=True)\n",
      "C:\\Users\\ABDULS~1\\AppData\\Local\\Temp/ipykernel_9692/4267138561.py:24: FutureWarning: In a future version of pandas all arguments of Series.dropna will be keyword-only.\n",
      "  Features['class'].dropna(\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "im here\n",
      "168\n",
      "classes\n",
      "['sure-not-crossing' 'unsure' 'sure-information-crossing']\n",
      "3\n",
      "unsure                       66\n",
      "sure-not-crossing            51\n",
      "sure-information-crossing    51\n",
      "Name: class, dtype: int64\n",
      "features\n",
      "168\n",
      "168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "filename = \"Home-Economic\"\n",
    "print(filename)\n",
    "modelname = \"\"\n",
    "inputname = \"\"\n",
    "q = pd.DataFrame(columns =[\"file\",\"model\",\"Accuracy\",\"Precision\",\"Recall\",\"F1\",\"input\"])\n",
    "\n",
    "Features = data.filter(['title','allinferences','heads-senti','allinferences-senti','class','senti-str-headline'], axis=1)\n",
    "#Features['heads-senti']= Features['heads-senti'].fillna(0)\n",
    "#Features['allinferences-senti']= Features['allinferences-senti'].fillna(0)\n",
    "#Features['allinferences']= Features['allinferences'].fillna(\"\")\n",
    "Features['title']= Features['title'].fillna(\"\")\n",
    "Features['senti-str-headline']= Features['senti-str-headline'].fillna(\"\")\n",
    "Features['allinferences']= Features['allinferences'].fillna(\"\")\n",
    "Features['class']= Features['class'].fillna(\"\")\n",
    "print(\"im here\")\n",
    "print(len(Features))    \n",
    "Features['title'] = Features['title'].dropna(\"\").reset_index(drop=True)\n",
    "print(len(Features)) \n",
    "Features['senti-str-headline'] = Features['senti-str-headline'].dropna(\"\").reset_index(drop=True)\n",
    "print(len(Features)) \n",
    "Features['allinferences'] = Features['allinferences'].dropna(\"\").reset_index(drop=True)\n",
    "print(len(Features)) \n",
    "Features['class'] = Features['class'].dropna(\"\").reset_index(drop=True)\n",
    "Features['class'].dropna(\"\")\n",
    "print(len(Features)) \n",
    "print(\"im here\")\n",
    "print(len(Features)) \n",
    "Features.dropna(inplace=True, subset=['class'])\n",
    "Features = Features[pd.notnull(Features['class'])]\n",
    "\n",
    "Features = Features[Features['class'] != None]\n",
    "Features = Features[Features['class'].notna()]\n",
    "index_names = Features[ Features['class'] == \"\"].index\n",
    "Features.drop(index_names, inplace = True)\n",
    "Features.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "index_names2 = Features[ Features['title'] == \"\"].index\n",
    "Features.drop(index_names2, inplace = True)\n",
    "Features.reset_index(drop=True, inplace=True)\n",
    " \n",
    "    \n",
    "if len(Features[\"class\"].unique()) == 2:\n",
    "    model = model2\n",
    "        \n",
    "elif len(Features[\"class\"].unique()) == 3:\n",
    "    model = model3\n",
    "\n",
    "Features['title'] = Features[\"title\"].astype(str) + Features[\"allinferences\"].astype(str)\n",
    "Features['title'] = Features[\"title\"].astype(str) + Features[\"senti-str-headline\"].astype(str)\n",
    "stop_words_l= stopwords.words('english')\n",
    "Features['title'].dropna().apply(lambda x: [item for item in x if item not in stop_words_l])\n",
    "Features['title'] = Features['title'].dropna().map(lambda x: x.lower())\n",
    "print(len(Features)) \n",
    "print(\"im here\")\n",
    "print(len(Features))\n",
    "print(\"classes\")\n",
    "print(Features[\"class\"].unique())\n",
    "print(len(Features[\"class\"].unique()))\n",
    "print(Features['class'].value_counts())\n",
    "train = Features.filter(['title'])\n",
    "test  = Features.filter(['class'])\n",
    "print(\"features\")\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "classes = label_encoder.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac74995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im here\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, classes.ravel(),test_size=0.2, shuffle = True, random_state=42,stratify=classes)\n",
    "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train.ravel(), test_size=0.25, random_state=42,stratify=y_train)\n",
    "Features = Features.filter(['title','class'])\n",
    "print(\"im here\")\n",
    "print(len(Features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "162efe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"obama, kerry urge all parties in turkey to support elected gov'tobama, kerry\", \"urge all parties in turkey to support elected gov't. consists of  friendly. want\", \"to get elected gov't to do the right thing. want to get elected gov't to do the\", \"right thing. becuase get elected gov't's support. has sub event get support from\", 'other parties. effected gets thanked. reacted happy. caused support for elected', \"gov't. it can be becuase the people of the country to support the gov't.\", 'isfilledby the president of the united states. intended to be helpful. wanted to', \"make sure the gov't is in power. wanted to make sure the gov't is in power.\", 'needed to meet with all parties in the country. 14the sentiment score is', 'positive']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "sen_w_feats = []\n",
    "labels2 = []\n",
    "labe = []\n",
    "for index, row in Features.iterrows():\n",
    "    sen_w_feats.append(row['title'])\n",
    "    labels2.append(row['class'])\n",
    "labe = label_encoder.fit_transform(test)\n",
    "print(wrap(sen_w_feats[1], 80))\n",
    "batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "epochs = 3\n",
    "max_len = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8d5d349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009\n",
      "168\n",
      "168\n",
      "0\n",
      "167\n",
      "134\n",
      "16\n",
      "18\n",
      "1009\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "for sent in sen_w_feats:\n",
    "    input_ids = tokenizer.encode(str(sent), add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "print(max_len)\n",
    "trs = int(0.8*len(Features))\n",
    "vas = int(0.1*len(Features))\n",
    "tes = len(Features)-(trs+vas)\n",
    "assert((trs+vas+tes) == len(Features))\n",
    "indeces = np.arange(0, len(Features))\n",
    "print(len(Features))\n",
    "print(len(indeces))\n",
    "random.shuffle(indeces)\n",
    "print(indeces.min(axis=0))\n",
    "print(indeces.max(axis=0))\n",
    "tr_idx  = indeces[0:trs]\n",
    "val_idx = indeces[trs:(trs+vas)]\n",
    "tes_idx = indeces[(trs+vas):]\n",
    "print(trs)\n",
    "print(vas)\n",
    "print(tes)\n",
    "print(max_len)\n",
    "print(len(sen_w_feats))\n",
    "max_len= max_len\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sent in sen_w_feats:\n",
    "    encoded_dict = tokenizer.encode_plus(str(sent), add_special_tokens=max_len, truncation= True, padding= 'max_length', return_attention_mask=True, return_tensors='pt',) \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2f9dad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "168\n",
      "168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids       = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labe)\n",
    "labels = labels.type(torch.LongTensor)\n",
    "print(len(input_ids))\n",
    "print(len(attention_masks))\n",
    "print(len(labels))\n",
    "tr_ds = TensorDataset(input_ids[tr_idx], attention_masks[tr_idx], labels[tr_idx])\n",
    "va_ds = TensorDataset(input_ids[val_idx], attention_masks[val_idx], labels[val_idx])\n",
    "te_ds = TensorDataset(input_ids[tes_idx], attention_masks[tes_idx], labels[tes_idx])\n",
    "train_dataloader = DataLoader(tr_ds, sampler=RandomSampler(tr_ds))\n",
    "validation_dataloader = DataLoader(va_ds, sampler=RandomSampler(va_ds))\n",
    "test_dataloader = DataLoader(te_ds, sampler=RandomSampler(te_ds))\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps = 1e-8)\n",
    "to_steps  = len(train_dataloader)*epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=to_steps)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "s_val = 42\n",
    "random.seed(s_val)\n",
    "np.random.seed(s_val)\n",
    "torch.manual_seed(s_val)\n",
    "torch.cuda.manual_seed_all(s_val)\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "831bb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    134.    Elapsed: 0:03:14.\n",
      "  Batch    80  of    134.    Elapsed: 0:06:45.\n",
      "  Batch   120  of    134.    Elapsed: 0:10:08.\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epcoh took: 0:11:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75\n",
      "  Validation Loss: 0.60\n",
      "  Validation took: 0:00:27\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    134.    Elapsed: 0:02:56.\n",
      "  Batch    80  of    134.    Elapsed: 0:05:49.\n",
      "  Batch   120  of    134.    Elapsed: 0:08:47.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:10:01\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75\n",
      "  Validation Loss: 0.58\n",
      "  Validation took: 0:00:28\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    134.    Elapsed: 0:02:54.\n",
      "  Batch    80  of    134.    Elapsed: 0:05:44.\n",
      "  Batch   120  of    134.    Elapsed: 0:08:37.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epcoh took: 0:09:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75\n",
      "  Validation Loss: 0.71\n",
      "  Validation took: 0:00:31\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()        \n",
    "        result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels, return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():        \n",
    "            result = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels,return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    training_stats.append({'epoch': epoch_i + 1,'Training Loss': avg_train_loss,'Valid. Loss': avg_val_loss, 'Valid. Accur.': avg_val_accuracy,'Training Time': training_time,'Validation Time': validation_time})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcc43c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Total training took 0:32:31 (h:mm:ss)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'o' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ABDULS~1\\AppData\\Local\\Temp/ipykernel_9692/416948469.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mpr\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_true_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"macro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mnew_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"BERT\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Precision\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Recall\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"F1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'o' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "model.eval()\n",
    "predictions , true_labels = [], [] \n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,return_dict=True)\n",
    "    logits = result.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "f1 = f1_score(flat_true_labels, flat_predictions, average=\"macro\")\n",
    "acs =accuracy_score(flat_true_labels, flat_predictions)\n",
    "f1 =f1_score(flat_true_labels, flat_predictions, average=\"macro\")\n",
    "re =recall_score(flat_true_labels, flat_predictions, average=\"macro\")\n",
    "pr =precision_score(flat_true_labels, flat_predictions, average=\"macro\")\n",
    "new_row = {\"file\":filename,\"model\":\"BERT\",\"Accuracy\":str(round(acs,2)),\"Precision\":str(round(pr,2)),\"Recall\":str(round(re,2)),\"F1\":str(round(f1,2))}\n",
    "o = o.append(new_row,ignore_index=True)\n",
    "q = q.append(new_row,ignore_index=True)\n",
    "q.to_csv(os.path.join(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d37cd0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'Home-Economic',\n",
       " 'model': 'BERT',\n",
       " 'Accuracy': '0.72',\n",
       " 'Precision': '0.65',\n",
       " 'Recall': '0.67',\n",
       " 'F1': '0.63'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81390b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
